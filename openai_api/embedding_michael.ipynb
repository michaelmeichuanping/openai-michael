{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "1.确保您按照[README](README-CN.md)中的说明在环境中设置了API密钥\n",
    "\n",
    "2.安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken openai pandas matplotlib plotly scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 生成 Embedding (基于 text-embedding-ada-002 模型)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入对于处理自然语言和代码非常有用，因为其他机器学习模型和算法（如聚类或搜索）可以轻松地使用和比较它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embedding](images/embedding-vectors.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 pandas 包。Pandas 是一个用于数据处理和分析的 Python 库\n",
    "# 提供了 DataFrame 数据结构，方便进行数据的读取、处理、分析等操作。\n",
    "import pandas as pd\n",
    "# 导入 tiktoken 库。Tiktoken 是 OpenAI 开发的一个库，用于从模型生成的文本中计算 token 数量。\n",
    "import tiktoken\n",
    "# 从 openai.embeddings_utils 包中导入 get_embedding 函数。\n",
    "# 这个函数可以获取 GPT-3 模型生成的嵌入向量。\n",
    "# 嵌入向量是模型内部用于表示输入数据的一种形式。\n",
    "from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 亚马逊美食评论数据集(amazon-fine-food-reviews)\n",
    "\n",
    "Source:[美食评论数据集](https://www.kaggle.com/snap/amazon-fine-food-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](images/amazon-fine-food-reviews.png)\n",
    "\n",
    "\n",
    "该数据集包含截至2012年10月用户在亚马逊上留下的共计568,454条美食评论。为了说明目的，我们将使用该数据集的一个子集，其中包括最近1,000条评论。这些评论都是用英语撰写的，并且倾向于积极或消极。每个评论都有一个产品ID、用户ID、评分、标题（摘要）和正文。\n",
    "\n",
    "我们将把评论摘要和正文合并成一个单一的组合文本。模型将对这个组合文本进行编码，并输出一个单一的向量嵌入。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset from .csv\n",
    "input_datapath = \"data/fine_food_reviews_1k.csv\"\n",
    "df = pd.read_csv(input_datapath, index_col=0)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select 6-colum\n",
    "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "\n",
    "# Remove all rows with NULL values from the DataFrame\n",
    "df = df.dropna()\n",
    "\n",
    "# 将 \"Summary\" 和 \"Text\" 字段组合成新的字段 \"combined\"\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    ")\n",
    "\n",
    "# print first 2 records\n",
    "df.head(2)\n",
    "\n",
    "# print \"combined\" colunm's 1st and last 5 entries\n",
    "df[\"combined\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 模型关键参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型类型\n",
    "# 建议使用官方推荐的第二代嵌入模型：text-embedding-ada-002\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "# text-embedding-ada-002 模型对应的分词器（TOKENIZER）\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "# text-embedding-ada-002 模型支持的输入最大 Token 数是8191，向量维度 1536\n",
    "# 在我们的 DEMO 中过滤 Token 超过 8000 的文本\n",
    "max_tokens = 8000  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将样本减少到最近的1,000个评论，并删除过长的样本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较搞笑的是彭老师给的原始数据就只有999行\n",
    "# 原来老师的script是：将样本减少到最近的1,000个评论，并删除过长的样本\n",
    "# 我修改为只提取500个评论\n",
    "\n",
    "\n",
    "# 设置要筛选的评论数量为1000\n",
    "top_n = 500\n",
    "# 对DataFrame进行排序，基于\"Time\"列，然后选取最后的2000条评论。\n",
    "# 这个假设是，我们认为最近的评论可能更相关，因此我们将对它们进行初始筛选。\n",
    "df = df.sort_values(\"Time\").tail(top_n * 2) \n",
    "# 丢弃\"Time\"列，因为我们在这个分析中不再需要它。\n",
    "df.drop(\"Time\", axis=1, inplace=True)\n",
    "# 从'embedding_encoding'获取编码\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# 计算每条评论的token数量。我们通过使用encoding.encode方法获取每条评论的token数，然后把结果存储在新的'n_tokens'列中。\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# 如果评论的token数量超过最大允许的token数量，我们将忽略（删除）该评论。\n",
    "# 我们使用.tail方法获取token数量在允许范围内的最后top_n（1000）条评论。\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "\n",
    "# 打印出剩余评论的数量。\n",
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成 Embeddings 并保存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 注意：如果未使用信用卡支付过 OpenAI 账单的同学，可以直接跳过此步骤。\n",
    "\n",
    "\n",
    "\n",
    "[OpenAI 速率限制](https://platform.openai.com/docs/guides/rate-limits/overview)\n",
    "\n",
    "对于免费试用用户的前48小时，我们已经添加了每日速率限制。正常的RPM和TPM限制仍然适用，但每个端点每天的请求次数也有单独的限制。\n",
    "\n",
    "目前，免费试用用户无法访问微调模型。\n",
    "\n",
    "需要注意的是，速率限制可以通过任一选项触发，取决于哪个先达到。例如，即使在这20个请求中没有发送150k个标记符号，在编辑端点上发送了20个只包含100个标记符号的请求也会填满您的限额。\n",
    "\n",
    "\n",
    "[OpenAI Rate Limits](https://platform.openai.com/docs/guides/rate-limits/overview)\n",
    "\n",
    "For free trial users in the first 48 hour, we have added per day rate limits. The normal RPM and TPM limits apply but there is also a separate limits on the number of requests per day for each endpoint.\n",
    "\n",
    "Free trial users also do not have access to fine-tune models at this time.\n",
    "\n",
    "It is important to note that the rate limit can be hit by either option depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Edit endpoint and that would fill your limit, even if you did not send 150k tokens within those 20 requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAPI to generate Embeddings\n",
    "# 实际生成会耗时几分钟\n",
    "# 提醒：为了规避有些同学没有OpenAI rateLimitde 问题， \n",
    "# 这一步可直接使用~/data/fine_food_reviews_with_embeddings_1k\n",
    "\n",
    "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model))\n",
    "\n",
    "# Verification\n",
    "df.head(2)\n",
    "df[\"embedding\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_datapath = \"data/fine_food_reviews_with_embeddings_1k_0904.csv\"\n",
    "df.to_csv(output_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.读取 fine_food_reviews_with_embeddings_1k 嵌入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑到未使用信用卡支付过 OpenAI 账单的同学会遇到ratelimit的问题\n",
    "# 从OpenAPI实现emebeding可能会遇到问题\n",
    "# 老师已经将包含embedding data的fine_food_reviews_with_embeddings_1k.csv 提前准备好了\n",
    "\n",
    "\n",
    "embedding_datapath = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n",
    "df_embedded = pd.read_csv(embedding_datapath, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看 Embedding 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_embedded[\"embedding\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_embedded[\"embedding\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded[\"embedding\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# 将字符串转换为向量\n",
    "df_embedded[\"embedding_vec\"] = df_embedded[\"embedding\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_embedded[\"embedding_vec\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_embedded[\"embedding_vec\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 t-SNE 可视化 1536 维 Embedding 美食评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 NumPy 包，NumPy 是 Python 的一个开源数值计算扩展。这种工具可用来存储和处理大型矩阵，\n",
    "# 比 Python 自身的嵌套列表（nested list structure)结构要高效的多。\n",
    "import numpy as np\n",
    "# 从 matplotlib 包中导入 pyplot 子库，并将其别名设置为 plt。\n",
    "# matplotlib 是一个 Python 的 2D 绘图库，pyplot 是其子库，提供了一种类似 MATLAB 的绘图框架。\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 从 sklearn.manifold 模块中导入 TSNE 类。\n",
    "# TSNE (t-Distributed Stochastic Neighbor Embedding) 是一种用于数据可视化的降维方法，尤其擅长处理高维数据的可视化。\n",
    "# 它可以将高维度的数据映射到 2D 或 3D 的空间中，以便我们可以直观地观察和理解数据的结构。\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_embedded[\"embedding_vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，确保你的嵌入向量都是等长的\n",
    "assert df_embedded['embedding_vec'].apply(len).nunique() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将嵌入向量列表转换为二维 numpy 数组\n",
    "matrix = np.vstack(df_embedded['embedding_vec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 t-SNE 模型，t-SNE 是一种非线性降维方法，常用于高维数据的可视化。\n",
    "# n_components 表示降维后的维度（在这里是2D）\n",
    "# perplexity 可以被理解为近邻的数量\n",
    "# random_state 是随机数生成器的种子\n",
    "# init 设置初始化方式\n",
    "# learning_rate 是学习率。\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 t-SNE 对数据进行降维，得到每个数据点在新的2D空间中的坐标\n",
    "vis_dims = tsne.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vis_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义了五种不同的颜色，用于在可视化中表示不同的等级\n",
    "colors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从降维后的坐标中分别获取所有数据点的横坐标和纵坐标\n",
    "x = [x for x,y in vis_dims]\n",
    "y = [y for x,y in vis_dims]\n",
    "\n",
    "# 根据数据点的评分（减1是因为评分是从1开始的，而颜色索引是从0开始的）获取对应的颜色索引\n",
    "color_indices = df_embedded.Score.values - 1\n",
    "\n",
    "# 确保你的数据点和颜色索引的数量匹配\n",
    "assert len(vis_dims) == len(df_embedded.Score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个基于预定义颜色的颜色映射对象\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "# 使用 matplotlib 创建散点图，其中颜色由颜色映射对象和颜色索引共同决定，alpha 是点的透明度\n",
    "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
    "\n",
    "# 为图形添加标题\n",
    "plt.title(\"Amazon ratings visualized in language using t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE降维后，产生了大约3个大类，其中1个大类的评论大多是负面的。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用 K-Means 聚类，然后使用 t-SNE 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 从 scikit-learn中导入 KMeans 类。KMeans 是一个实现 K-Means 聚类算法的类。\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# np.vstack 是一个将输入数据堆叠到一个数组的函数（在垂直方向）。\n",
    "# 这里它用于将所有的 ada_embedding 值堆叠成一个矩阵。\n",
    "# matrix = np.vstack(df.ada_embedding.values)\n",
    "\n",
    "# 定义要生成的聚类数。\n",
    "n_clusters = 4\n",
    "\n",
    "# 创建一个 KMeans 对象，用于进行 K-Means 聚类。\n",
    "# n_clusters 参数指定了要创建的聚类的数量；\n",
    "# init 参数指定了初始化方法（在这种情况下是 'k-means++'）；\n",
    "# random_state 参数为随机数生成器设定了种子值，用于生成初始聚类中心。\n",
    "# n_init=10 消除警告 'FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4'\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42, n_init=10)\n",
    "\n",
    "# 使用 matrix（我们之前创建的矩阵）来训练 KMeans 模型。这将执行 K-Means 聚类算法。\n",
    "kmeans.fit(matrix)\n",
    "\n",
    "# kmeans.labels_ 属性包含每个输入数据点所属的聚类的索引。\n",
    "# 这里，我们创建一个新的 'Cluster' 列，在这个列中，每个数据点都被赋予其所属的聚类的标签。\n",
    "df_embedded['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先为每个聚类定义一个颜色。\n",
    "colors = [\"red\", \"green\", \"blue\", \"purple\"]\n",
    "\n",
    "# 然后，你可以使用 t-SNE 来降维数据。这里，我们只考虑 'embedding_vec' 列。\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "vis_data = tsne_model.fit_transform(matrix)\n",
    "\n",
    "# 现在，你可以从降维后的数据中获取 x 和 y 坐标。\n",
    "x = vis_data[:, 0]\n",
    "y = vis_data[:, 1]\n",
    "\n",
    "# 'Cluster' 列中的值将被用作颜色索引。\n",
    "color_indices = df_embedded['Cluster'].values\n",
    "\n",
    "# 创建一个基于预定义颜色的颜色映射对象\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "# 使用 matplotlib 创建散点图，其中颜色由颜色映射对象和颜色索引共同决定\n",
    "plt.scatter(x, y, c=color_indices, cmap=colormap)\n",
    "\n",
    "# 为图形添加标题\n",
    "plt.title(\"Clustering visualized in 2D using t-SNE\")\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-MEANS 聚类可视化效果，4类：一个专注于狗粮，一个专注于负面评论，两个专注于正面评论。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用 Embedding 进行文本搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cosine](images/cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarity 函数计算两个嵌入向量之间的余弦相似度。\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_embedded[\"embedding_vec\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个名为 search_reviews 的函数，\n",
    "# Pandas DataFrame 产品描述，数量，以及一个 pprint 标志（默认值为 True）。\n",
    "def search_reviews(df, product_description, n=3, pprint=True):\n",
    "    product_embedding = get_embedding(\n",
    "        product_description,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarity\"] = df.embedding_vec.apply(lambda x: cosine_similarity(x, product_embedding))\n",
    "\n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "        .head(n)\n",
    "        .combined.str.replace(\"Title: \", \"\")\n",
    "        .str.replace(\"; Content:\", \": \")\n",
    "    )\n",
    "    if pprint:\n",
    "        for r in results:\n",
    "            print(r[:200])\n",
    "            print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 'delicious beans' 作为产品描述和 3 作为数量，\n",
    "# 调用 search_reviews 函数来查找与给定产品描述最相似的前3条评论。\n",
    "# 其结果被存储在 res 变量中。\n",
    "res = search_reviews(df_embedded, 'delicious beans', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_reviews(df_embedded, 'dog food', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_reviews(df_embedded, 'awful', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "def search_reviews(df, product_description, n=3, pprint=True):\n",
    "    product_embedding = get_embedding(\n",
    "        product_description,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarity\"] = df.embedding_vec.apply(lambda x: cosine_similarity(x, product_embedding))\n",
    "\n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "        .head(n)\n",
    "        .combined.str.replace(\"Title: \", \"\")\n",
    "        .str.replace(\"; Content:\", \": \")\n",
    "    )\n",
    "    if pprint:\n",
    "        for r in results:\n",
    "            print(r[:200])\n",
    "            print()\n",
    "    return results\n",
    "\n",
    "res = search_reviews(df_embedded, 'dog food', n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
